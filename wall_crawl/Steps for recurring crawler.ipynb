{
 "metadata": {
  "name": "Steps for recurring crawler"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "from boto.s3.connection import S3Connection", 
      "from boto.s3.key import Key", 
      "from navigate_db import PySql", 
      "import MySQLdb as mysql", 
      "from crawl_tools import crawl_feed,check_if_crawled"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 4
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# connect to s3 database", 
      "conn = S3Connection('AKIAJDIWDVVGWXFOSPEQ', 'RpcwFl6tw2XtOqnwbhXK9PemhUQ8kK6UdCMJ5GaI')", 
      "main_bucket = conn.get_bucket('fbcrawl1')", 
      "token_bucket = conn.get_bucket('fbtokens')", 
      "realtime_bucket = conn.get_bucket('fbrealtime')"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 9
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "# edgeflip databse", 
      "con = mysql.connect('edgeflip-db.efstaging.com','root','9uDTlOqFmTURJcb','edgeflip')", 
      "cur = con.cursor()", 
      "orm = PySql(cur)", 
      "most_data = orm.query('select fbid,appid,ownerid,token from tokens')"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 18
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "def always_crawl():", 
      "    # connect to s3 database", 
      "    conn = S3Connection('AKIAJDIWDVVGWXFOSPEQ', 'RpcwFl6tw2XtOqnwbhXK9PemhUQ8kK6UdCMJ5GaI')", 
      "    main_bucket = conn.get_bucket('fbcrawl1')", 
      "    token_bucket = conn.get_bucket('fbtokens')", 
      "    realtime_bucket = conn.get_bucket('fbrealtime')", 
      "", 
      "    # edgeflip databse", 
      "    con = mysql.connect('edgeflip-db.efstaging.com','root','9uDTlOqFmTURJcb','edgeflip')", 
      "    cur = con.cursor()", 
      "    orm = PySql(cur)", 
      "    most_data = orm.query('select fbid,appid,ownerid,token from tokens')", 
      "", 
      "    new_count = 0", 
      "    update_count = 0", 
      "    for item in most_data:", 
      "        fbid = item[0]", 
      "        appid = item[1]", 
      "        ownerid = item[2]", 
      "        token = item[3]", 
      "        main_key = fbid+','+ownerid", 
      "        if not main_bucket.lookup(main_key):", 
      "            # crawl_feed returns a ", 
      "            response = crawl_feed(fbid,token)", 
      "            k = main_bucket.new_key()", 
      "            k.key = main_key", 
      "            k.set_contents_from_string(response)", 
      "            # put the fbid,ownerid, and token in token_bucket", 
      "            token_key = token_bucket.new_key()", 
      "            token_key.key = fbid", 
      "            token_key_struct = {fbid: [{owerid: token}]}", 
      "            jsoned = json.dumps(token_key_struct)", 
      "            token_key.set_contents_from_string(jsoned)", 
      "            new_count += 1", 
      "            # otherwise we've already crawled our user and there should be information about", 
      "            # him/her in our main_bucket and our token_bucket", 
      "        else:", 
      "        # get everything from the subscribed updates", 
      "            if realtime_bucket.lookup(fbid):", 
      "                cur = realtime_bucket.get_key(fbid)", 
      "                data = json.loads(cur.get_contents_as_string())", 
      "                update_time = data['time']", 
      "                tokens = get_tokens_for_user(fbid, token_bucket)", 
      "                for ownerid, cur_token in tokens:", 
      "                    api = 'https://graph.facebook.com/{0}?fields=feed.since({1})&access_token={2}'", 
      "                    this_api = api.format(fbid,update_time,cur_token)", 
      "                    this_response = json.loads(urllib2.urlopen(this_api).read())", 
      "                    # data already stored related to user", 
      "                    pertaining_key = main_bucket.get_key(fbid+','+ownerid)", 
      "                    pertaining_data = json.loads(pertaining_key.get_contents_as_string())", 
      "                    # update the already stored data with the newly acquired data", 
      "                    # remember to convert to a json string to store in our s3 bucket", 
      "                    new_data = json.dumps(pertaining_data.update(this_response))", 
      "                    pertaining_key.set_contents_from_string(new_data)", 
      "                    udpate_count += 1", 
      "                    # if our current fbid isn't in the RealTime updates bucket, move along", 
      "            else:", 
      "                pass", 
      "    print \"%s new users added and %s users ", 
      "    ", 
      "                ", 
      "                ", 
      "                ", 
      "            ", 
      "            ", 
      "        ", 
      "         ", 
      "        "
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 16, 
       "text": [
        "['fbid', 'appid', 'ownerid', 'token', 'expires', 'updated']"
       ]
      }
     ], 
     "prompt_number": 16
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [], 
     "language": "python", 
     "outputs": []
    }
   ]
  }
 ]
}