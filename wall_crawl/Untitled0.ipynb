{
 "metadata": {
  "name": "Untitled0"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "def always_crawl_from_database(tool,crawl_timestamp = None):", 
      "    import csv", 
      "    # connect to s3 database", 
      "    conn = S3Connection('AKIAJDIWDVVGWXFOSPEQ', 'RpcwFl6tw2XtOqnwbhXK9PemhUQ8kK6UdCMJ5GaI')", 
      "    main_bucket = conn.get_bucket('fbcrawl1')", 
      "    token_bucket = conn.get_bucket('fbtokens')", 
      "    realtime_bucket = conn.get_bucket('fbrealtime')", 
      "    if not crawl_timestamp:", 
      "        most_data = tool.query('select fbid,ownerid,token from tokens')", 
      "    else:", 
      "\tmost_data = tool.query('select fbid,ownerid,token from tokens where updated > FROM_UNIXTIME(%s)' % crawl_timestamp)", 
      "    crawl_log = open('crawl_log.csv','wb')", 
      "    crawl_log_writer = csv.writer(crawl_log,delimiter=',')", 
      "    new_count = 0", 
      "    # under the new structure", 
      "    all_data_from_s3 = main_bucket.get_key('allfeeds')", 
      "    if all_data_from_s3 == None:", 
      "        all_feeds = {'data': []}", 
      "        all_tokens = {'data': []}", 
      "        for item in most_data:", 
      "            fbid = str(item[0])", 
      "            ownerid = str(item[1])", 
      "            token = item[2]", 
      "            # check if our fbid is in all_tokens if not add it if so append pertinent info", 
      "            if fbid in [i.keys()[0] for i in all_tokens['data']]:", 
      "                all_tokens['data'][fbid].append((ownerid,token))", 
      "            else:", 
      "                all_tokens['data'].append({fbid: [(ownerid,token)]})", 
      "            main_key = fbid + ',' + ownerid", 
      "            # go ahead and write the fbid to the csv log file", 
      "            csv_log_writer.writerow([fbid])", 
      "            try:", 
      "                feed = crawl_feed(fbid,token)", 
      "            except urllib2.HTTPError:", 
      "                feed = ''", 
      "            # make the new feed structure keyed by the main_key and append it to our", 
      "            # all_feeds data structure", 
      "            this_feed = {main_key: feed}", 
      "            all_feeds['data'].append(this_feed)", 
      "            new_count += 1", 
      "    else:", 
      "        all_feeds = json.loads(all_data_from_s3.get_contents_as_string())", 
      "        token_stuff = token_bucket.get_key('fb_tokens')", 
      "        if token_stuff == None:", 
      "            all_tokens = {'data': []}", 
      "        else:", 
      "            all_tokens = json.loads(token_stuff.get_contents_as_string())", 
      "            ", 
      "        for item in most_data:", 
      "            fbid = str(item[0])", 
      "            ownerid = str(item[1])", 
      "            token = item[2]", 
      "            # check if the fbid is in all_tokens if so append else make new entry", 
      "            if fbid in [i.keys()[0] for i in all_tokens['data']]:", 
      "                all_tokens['data'][fbid].append((ownerid,token))", 
      "            else:", 
      "                all_tokens['data'].append({fbid:[(ownerid,token)]})", 
      "                ", 
      "            main_key = fbid+','+ownerid", 
      "\t    # go ahead and write the fbid to the csv log file", 
      "\t    crawl_log_writer.writerow([fbid])", 
      "            # crawl_feed returns a json blob of the users feed", 
      "\t    # on this pass of the code we are getting the entire feed", 
      "\t    try: ", 
      "            \tfeed = crawl_feed(fbid,token)", 
      "\t    except urllib2.HTTPError:", 
      "\t\tfeed = ''", 
      "            this_feed = {main_key: feed}", 
      "            all_feeds['data'].append(this_feed)", 
      "            ", 
      "            new_count += 1", 
      "        # otherwise we've already crawled our user and there should be information about", 
      "        # him/her in our main_bucket and our token_bucket", 
      "        else:", 
      "        # get everything from the subscribed updates with the next method's execution", 
      "\t    pass", 
      "    return new_count", 
      "", 
      "", 
      "", 
      " "
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "def crawl_realtime_updates2(tool):", 
      "\tconn = S3Connection('AKIAJDIWDVVGWXFOSPEQ', 'RpcwFl6tw2XtOqnwbhXK9PemhUQ8kK6UdCMJ5GaI')", 
      "\tmain_bucket = conn.get_bucket('fbcrawl1')", 
      "\ttoken_bucket = conn.get_bucket('fbtokens')", 
      "\trealtime_bucket = conn.get_bucket('fbrealtime')", 
      "\tapi = 'https://graph.facebook.com/{0}?fields=feed.since({1})&access_token={2}'", 
      "\t# get all the realtime update keys so we can parse through them and grab the updates", 
      "\t_time = time.time()", 
      "\tdata = realtime_bucket.get_key('data')", 
      "\t# keep track of which users we've crawled on this pass and make sure to not crawl", 
      "\t# them twice or else we will have duplicate information in the database", 
      "\t# users_crawled will also have pre-included fbids from the always_crawl_from_database algorithm", 
      "\t# which generates a crawled log of fbids from it's execution in order to avoid duplicate crawling", 
      "\tusers_crawled = []", 
      "\treader = csv.reader(open('crawl_log.csv','r'),delimiter=',')", 
      "\t# read all the fbids from our file and add them to users_crawled", 
      "\ttry:", 
      "\t\twhile True:", 
      "\t\t\tusers_crawled.append(reader.next()[0])", 
      "\texcept StopIteration:", 
      "\t\tpass", 
      "        # go ahead and read in the token_bucket key with all our tokens", 
      "        token_key = token_bucket.get_key('fb_tokens')", 
      "        all_tokens = json.loads(token_key.get_contents_as_string())", 
      "        ", 
      "\tfor fbid, update_time in data['data']:", 
      "            # if fbid wasn't crawled in always_crawl_from_database based on users_crawled which read in what was crawled....", 
      "            if fbid not in users_crawled:", 
      "                # get all the tokens for the fbid and crawl him/her with all tokens and store into bucket", 
      "                # if we don't have our fbid in our tokens key in the tokens bucket we will get all", 
      "                # that fbid's tokens and add them to the key in the pertinent bucket and simultaneously crawl", 
      "                # his/her feed with these tokens", 
      "                if fbid not in [i.keys()[0] for i in all_tokens['data']]:", 
      "                    user_tokens_stuff = tool.query(\"select ownerid, token from tokens where fbid='%s'\" % fbid)", 
      "                    token_data_struct = {fbid: []}", 
      "                    for each in user_token_stuff:", 
      "                        owner_id = str(each[0])", 
      "                        token = each[1]", 
      "                        token_data_struct[fbid].append((owner_id,token))", 
      "                        main_key = fbid+','+owner_id", 
      "                        # try getting our old feed, if we have it use it if we don't just add what we get", 
      "                        # back from crawl_feed", 
      "                        ", 
      "                        ", 
      "            ", 
      "            ", 
      "\t# delete all the obsolete keys that we've crawled that were created earlier than", 
      "\t# this algorithm was invoked", 
      "\trealtime_bucket.delete_keys([key for key in realtime_bucket if int(key) < _time]) ", 
      "\tprint \"Realtime updates added to s3\"\t"
     ], 
     "language": "python", 
     "outputs": []
    }
   ]
  }
 ]
}